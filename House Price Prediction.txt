### House Price Prediction 
### By Ashima Pillai 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


#delete unnecessary column
#Normalized the data with mean

data = pd.read_csv('Housing.csv')
data = data.drop(['airconditioning','basement','furnishingstatus','guestroom','hotwaterheating','mainroad','prefarea'],axis=1)
mean = data.mean()[0]
stddev = data.std()[0]
data = (data - data.mean())/data.std()

data.head()

#Separate out the Feature and Target matrices and device test and train data with 0.9 split ration

data = np.asarray(data)
Y=data[:,0:1]
X=data[:,1:]
one = np.ones((len(X),1))
X=np.concatenate((one,X),axis=1)
split_ratio = 0.9
split = int(split_ratio*X.shape[0])
X_test=X[split+1:,:]
X_train = X[:split+1,:]
Y_test = Y[split+1:,:]
Y_train = Y[:split+1,:]

#Compute cost function
#denormalise price function
#compute error function
#plot graph function

#helper Functions
def computeCost(X,y,theta,lam):
    tobesummed = np.power(((X.dot(theta.T))-y),2)+lam*np.sum(np.power(theta,2))
    return np.sum(tobesummed)/(2*len(X))

def denormalise_price(price):
    global mean
    global stddev
    ret =price* stddev + mean
    return ret

def computeError(predicted,actual):
    error=0
    for i in range(len(predicted)):
        error +=abs(actual[i]-predicted[i])/actual[i]
        error /=len(actual)
        return error[0]

def plotGraph(x,y,labelX='X',labelY='Y',title='X vs Y'):
        fig, ax=plt.subplots()
        ax.plot(x,y,'r')
        ax.set_xlabel(labelX)
        ax.set_ylabel(labelY)
        ax.set_title(title)

#Gradient Descent
def gradientDescent(X,y,theta,iters,alpha,lam):
    lam_matrix = lam*np.ones(theta.shape)
    lam_matrix[0][0]=0
    for i in range(iters):
        theta = theta*(1- lam_matrix / len(X)) - (alpha/len(X)) * np.sum(X *(X @ theta.T - y),axis=0)
        
        
        return theta

#Normal Equation 
def normalEquation(X,Y,lam):
    lam_matrix = lam * np.identity(X.shape[1])
    lam_matrix[0][0] = 0
    theta = np.linalg.inv(X.T.dot(X) + lam_matrix).dot(X.T).dot(Y)
    return theta


theta = np.zeros([1,X.shape[1]])
alpha = .1 #learning rate
iters = 500 #epoch
error_matrix =[]
lam_range = 600 #maximum lambda
for lam in range(lam_range):
    g=gradientDescent(X_train,Y_train,theta,iters,alpha,lam)
    Cost = computeCost(X_train,Y_train,g,lam)
    
    Y_pred =X_test.dot(g.T)
    error = computeError(denormalise_price(Y_pred),denormalise_price (Y_test))
    error_matrix.append(error*100)
    optimal_lamda = 0
    min_error = 9999
    for i in range(len(error_matrix)):
        if error_matrix[i] < min_error:
            optimal_lamda = i
            min_error = error_matrix[i]
            
print("min Error :",(min_error),'%')
print("Optimal Lamda :",optimal_lamda)
plotGraph(np.arange(lam_range),error_matrix,'lamda','error','lamda vs error')


print('pred price =',denormalise_price(Y_pred[2][0]),'actual price =',denormalise_price(Y_test[2])[0])

error_mat = []
lam_range = 600

for lam in range(lam_range):
    theta = normalEquation(X,Y,lam)
    Cost = computeCost(X_train,Y_train,theta.T,lam)
    Y_pred = X_test.dot(theta)
    error = computeError(denormalise_price(Y_pred),denormalise_price(Y_test))
    error = computeError(denormalise_price(Y_pred),denormalise_price(Y_test))
    error_mat.append(error*100)
    
optimal_lamda = 0
min_error = 9999
for i in range(len(error_mat)):
    if error_mat[i] < min_error:
        optimal_lamda = i
        min_error = error_mat[i]
print("min Error :",min_error)
print("Optimal Lamda :", optimal_lamda)
plotGraph(np.arange(lam_range),error_mat,'lamda','error','lamda vs error')

print('pred price =',denormalise_price(Y_pred[2][0]),'actual price =',denormalise_price(Y_test[2])[0])





